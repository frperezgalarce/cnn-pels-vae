{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984721ea-2ed6-4933-a238-3a98989b3776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "new_directory = '/home/franciscoperez/Documents/GitHub/CNN-PELSVAE2/cnn-pels-vae/'\n",
    "os.chdir(new_directory)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from src.cnn.focalloss import  FocalLossMultiClass as focal_loss\n",
    "import src.utils as utils \n",
    "from src.cnn.training_cnn import initialize_masks, train_one_epoch_alternative, create_dataloader, setup_torch_environment, initialize_optimizers\n",
    "from src.sampler.getbatch import SyntheticDataBatcher\n",
    "\n",
    "from src.utils import get_data\n",
    "import yaml \n",
    "import numpy as np\n",
    "from typing import Union, Tuple, Optional, Any, Dict, List\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.optim as optim\n",
    "\n",
    "with open('src/configuration/regressor.yaml', 'r') as file:\n",
    "    config_file: Dict[str, Any] = yaml.safe_load(file)\n",
    "vae_model: str =   config_file['model_parameters']['ID']  \n",
    "data_sufix: str =   config_file['model_parameters']['sufix_path']  \n",
    "\n",
    "with open('src/configuration/nn_config.yaml', 'r') as file:\n",
    "    nn_config = yaml.safe_load(file)\n",
    "    \n",
    "PP = utils.load_pp_list(vae_model)\n",
    "prior = False\n",
    "create_samples = True\n",
    "wandb_active = False\n",
    "N_LAYERS = 3\n",
    "opt_method= \"oneloss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c2924-0ecf-4e2d-bb1a-97bb59983213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network (CNN) for processing light curves.\n",
    "    \n",
    "    Attributes:\n",
    "        layers (int): Number of convolutional layers in the network.\n",
    "        conv1, conv2, ..., conv4 (nn.Conv1d): Convolutional layers of the network.\n",
    "        bn1, bn2, ..., bn4 (nn.BatchNorm1d): Batch normalization layers.\n",
    "        pool1, pool2, ..., pool4 (nn.MaxPool1d): Pooling layers to reduce spatial dimensions.\n",
    "        fc1 (nn.Linear): Fully connected layer to map features to intermediate representation.\n",
    "        fc2 (nn.Linear): Final fully connected layer to map intermediate representation to class scores.\n",
    "    \n",
    "    Parameters:\n",
    "        num_classes (int): Number of classes in the output prediction. Default is 2.\n",
    "        layers (int): Number of convolutional layers to use (2 to 4). Default is 2.\n",
    "        kernel_size (int): Size of the convolutional kernel. Default is 6.\n",
    "        stride (int): Stride of the convolution operation. Default is 1.\n",
    "    \n",
    "    Methods:\n",
    "        forward(x): Defines the forward pass of the CNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 2, layers = 2, \n",
    "                 kernel_size = 6, stride = 1, loss_function='focalLoss') -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CNN model with the given parameters.\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=kernel_size, \n",
    "                               stride=stride, padding=int(kernel_size/2), \n",
    "                               padding_mode='replicate', groups=2)\n",
    "\n",
    "        init.xavier_uniform_(self.conv1.weight)  \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(3)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=int(kernel_size/2), \n",
    "                               padding_mode='replicate', groups=2)\n",
    "\n",
    "        init.xavier_uniform_(self.conv2.weight)  \n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(3)\n",
    "\n",
    "        if self.layers > 2: \n",
    "            self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, \n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride, padding=int(kernel_size/2), \n",
    "                                   padding_mode='replicate', groups=2)\n",
    "\n",
    "            init.xavier_uniform_(self.conv3.weight)  \n",
    "            self.bn3 = nn.BatchNorm1d(64)\n",
    "            self.pool3 = nn.MaxPool1d(3)  \n",
    "        \n",
    "        if self.layers > 3: \n",
    "            self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, \n",
    "                                   kernel_size=kernel_size, \n",
    "                                   stride=stride, padding=int(kernel_size/2), \n",
    "                                   padding_mode='replicate', groups=2)\n",
    "\n",
    "            init.xavier_uniform_(self.conv4.weight)  \n",
    "            self.bn4 = nn.BatchNorm1d(128)\n",
    "            self.pool4 = nn.MaxPool1d(3)  \n",
    "\n",
    "        if self.layers == 2:\n",
    "            self.fc1 = nn.Linear(1056, 200)\n",
    "            init.xavier_uniform_(self.fc1.weight)  \n",
    "        elif self.layers == 3:\n",
    "            self.fc1 = nn.Linear(704, 200)\n",
    "            init.xavier_uniform_(self.fc1.weight) \n",
    "        elif self.layers == 4:\n",
    "            self.fc1 = nn.Linear(512, 200)\n",
    "            init.xavier_uniform_(self.fc1.weight) \n",
    "        \n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "        init.xavier_uniform_(self.fc2.weight)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the CNN.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input data tensor with shape (batch_size, channels, length).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor with shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)        \n",
    "\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        if self.layers == 3: \n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool3(x)\n",
    "            \n",
    "        if self.layers == 4: \n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool3(x)\n",
    "            \n",
    "            x = self.conv4(x)\n",
    "            x = self.bn4(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool4(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if (self.loss_function=='NLLLoss') or (self.loss_function=='focalLoss'):\n",
    "            return F.log_softmax(x, dim=1)  \n",
    "        else: \n",
    "            return x\n",
    "        \n",
    "def setup_model(num_classes: int, show_architecture: bool = True) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Setup and initialize the CNN model with the specified number of output classes and \n",
    "    configuration.\n",
    "\n",
    "    Parameters:\n",
    "        num_classes (int): Number of classes for the final output layer of the CNN.\n",
    "        device (torch.device): The device (CPU or GPU) where the model \n",
    "        should be allocated. show_architecture (bool): If True, print the \n",
    "        architecture of the model. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The initialized CNN model, potentially wrapped in a nn.DataParallel \n",
    "        module if multiple GPUs are available.\n",
    "\n",
    "    This function loads configuration from a YAML file, initializes a CNN model accordin\n",
    "    to this configuration, and moves the model to the specified device. If multiple GPUs \n",
    "    are available, it wraps the model in a nn.DataParallel module to enable parallel \n",
    "    processing.\n",
    "    \"\"\"\n",
    "    # Load neural network configuration from YAML files\n",
    "    nn_config = load_yaml_files(nn_config=True, regressor=False)\n",
    "\n",
    "    print('----- model setup --------')\n",
    "    # Initialize the CNN model with parameters from the configuration file\n",
    "    model = CNN(num_classes=num_classes, layers=nn_config['training']['layers'], loss_function=nn_config['training']['loss'])\n",
    "\n",
    "    # Move the model to the specified device (CPU or GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "\n",
    "    # If more than one GPU is available, use DataParallel for parallel processing\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # Optionally print the model architecture\n",
    "    if show_architecture:\n",
    "        print(\"Model Architecture:\")\n",
    "        print(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_yaml_files(nn_config: bool = True, regressor: bool = True):\n",
    "    \"\"\"\n",
    "    Load configuration data from YAML files based on the specified options.\n",
    "\n",
    "    Parameters:\n",
    "        nn_config (bool): Flag indicating whether to load the neural network \n",
    "                          configuration file. Default is True.\n",
    "        regressor (bool): Flag indicating whether to load the regressor configuration \n",
    "                          file. This flag is only considered if `nn_config` is also True. \n",
    "                          Default is True.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the loaded configurations as dictionaries.\n",
    "        - If both `nn_config` and `regressor` are True, returns a tuple \n",
    "          with both configurations.\n",
    "        - If only `nn_config` is True, returns a single-element tuple with the neural \n",
    "          network configuration.\n",
    "        - Returns None if `nn_config` is False.\n",
    "\n",
    "    This function reads configuration settings from 'src/configuration/nn_config.yaml' and optionally from\n",
    "    'src/configuration/regressor.yaml'. The returned configurations are used to set up and \n",
    "    customize the behavior of neural network models and training processes.\n",
    "    \"\"\"\n",
    "    if nn_config and regressor:\n",
    "        with open('src/configuration/nn_config.yaml', 'r') as file:\n",
    "            nn_config_dict = yaml.safe_load(file)\n",
    "\n",
    "        with open('src/configuration/regressor.yaml', 'r') as file:\n",
    "            regressor_dict = yaml.safe_load(file)\n",
    "        \n",
    "        print('------ Data loading -------------------')\n",
    "        print('mode: ', nn_config_dict['data']['mode_running'], nn_config_dict['data']['sample_size'])\n",
    "\n",
    "        return nn_config_dict, regressor_dict\n",
    "\n",
    "    elif nn_config:\n",
    "        with open('src/configuration/nn_config.yaml', 'r') as file:\n",
    "            nn_config_dict = yaml.safe_load(file)\n",
    "\n",
    "        return nn_config_dict\n",
    "    else: \n",
    "        raise Exception(\"Files were not loaded, please check function arguments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139f786-bb3c-4dcd-94cf-ca2d69afcf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_config, config_file = load_yaml_files(nn_config=True, regressor=True)\n",
    "\n",
    "vae_model: str = config_file['model_parameters']['ID']\n",
    "\n",
    "x_train, x_test, y_train, y_test, x_val, y_val, \\\n",
    "label_encoder, y_train_labeled, y_test_labeled = utils.get_data(nn_config['data']['sample_size'], \n",
    "                                                          nn_config['data']['mode_running'])\n",
    "\n",
    "class_weights, num_classes, _  = get_counts_and_weights_by_class(y_train_labeled, \n",
    "                                                    y_test_labeled, x_train)\n",
    "\n",
    "model = setup_model(num_classes, device)\n",
    "wset.setup_gradients(wandb_active, model)\n",
    "\n",
    "training_data = utils.move_data_to_device((x_train, y_train), device)\n",
    "val_data = utils.move_data_to_device((x_val, y_val), device)\n",
    "testing_data = utils.move_data_to_device((x_test, y_test), device)\n",
    "\n",
    "best_val = np.iinfo(np.int64).max\n",
    "harder_samples = True\n",
    "no_improvement_count, counter, weight_f1_score_hyperparameter_search  = 0, 0, 0\n",
    "train_loss_values, val_loss_values, train_accuracy_values, \\\n",
    "                                    val_accuracy_values  = [], [], [], []\n",
    "\n",
    "nn_config, config_file = wset.cnn_hyperparameters(wandb_active, hyperparam_opt, \n",
    "                                                  nn_config, config_file)\n",
    "\n",
    "train_dataloader = create_dataloader(training_data, nn_config['training']['batch_size'])\n",
    "val_dataloader = create_dataloader(val_data, nn_config['training']['batch_size'])\n",
    "test_dataloader = create_dataloader(testing_data, nn_config['training']['batch_size'])\n",
    "\n",
    "criterion, criterion_synthetic_samples = get_criterion(nn_config, class_weights)\n",
    "\n",
    "beta_actual = nn_config['training']['beta_initial']\n",
    "\n",
    "optimizer1, optimizer2, locked_masks, \\\n",
    "            locked_masks2 = initialize_optimizers(model, nn_config_dict = nn_config)\n",
    "\n",
    "batcher = SyntheticDataBatcher(pp = pp, vae_model=vae_model, \n",
    "                              n_samples=nn_config['training']['synthetic_samples_by_class'],\n",
    "                            seq_length = x_train.size(-1), prior=prior)\n",
    "\n",
    "for epoch in range(nn_config['training']['epochs']):\n",
    "    print(nn_config['training']['opt_method'], create_samples, harder_samples, \n",
    "        counter, nn_config['training']['ranking_method'])\n",
    "\n",
    "    if (nn_config['training']['opt_method']=='twolosses' \n",
    "        and create_samples and harder_samples):\n",
    "        dict_priorization = {}\n",
    "\n",
    "        if (nn_config['training']['ranking_method']=='no_priority') or (epoch < 2):\n",
    "            synthetic_data_loader = batcher.create_synthetic_batch(b=beta_actual, \n",
    "                                            wandb_active=wandb_active, \n",
    "                                            n_oversampling=nn_config['training']['n_oversampling'])\n",
    "\n",
    "        elif nn_config['training']['ranking_method']=='proportion':\n",
    "            ranking, proportions = get_dict_class_priorization(model, \n",
    "                                                    train_dataloader, \n",
    "                                                    ranking_method = \n",
    "                                                    nn_config['training']['ranking_method'])\n",
    "\n",
    "\n",
    "            proportions = ((proportions - np.min(proportions))/\n",
    "                          (np.max(proportions) - np.min(proportions))*16 + 8)\n",
    "\n",
    "            counter2 = 0\n",
    "\n",
    "            for o in ranking:\n",
    "                dict_priorization[label_encoder[o]] =  int(proportions[counter2])\n",
    "                counter2 = counter2 + 1\n",
    "\n",
    "            synthetic_data_loader = batcher.create_synthetic_batch(b=beta_actual, \n",
    "                                            wandb_active=wandb_active, \n",
    "                                            samples_dict = dict_priorization, \n",
    "                                            n_oversampling=nn_config['training']['n_oversampling'])            \n",
    "        else:\n",
    "            ranking, _ = get_dict_class_priorization(model, train_dataloader, \n",
    "                                                    ranking_method = \n",
    "                                                    nn_config['training']['ranking_method'])\n",
    "\n",
    "            ranking_penalization = 1.25\n",
    "            for o in ranking:\n",
    "                objects = nn_config['training']['synthetic_samples_by_class']*ranking_penalization\n",
    "                dict_priorization[label_encoder[o]] =  int(objects)\n",
    "                if ranking_penalization>0.5:\n",
    "                    ranking_penalization = ranking_penalization/1.25\n",
    "\n",
    "            synthetic_data_loader = batcher.create_synthetic_batch(b=beta_actual, \n",
    "                                            wandb_active=wandb_active, \n",
    "                                            samples_dict = dict_priorization,\n",
    "                                            n_oversampling = nn_config['training']['n_oversampling'])\n",
    "\n",
    "        beta_actual = 0.85 + 0.15 * np.exp(-0.1 * epoch)\n",
    "        harder_samples = False\n",
    "\n",
    "    elif  nn_config['training']['opt_method']=='twolosses' and create_samples: \n",
    "        print(\"Using available synthetic data\")\n",
    "    else:\n",
    "        print(\"Skipping synthetic sample creation\")\n",
    "        synthetic_data_loader = None\n",
    "\n",
    "    running_loss, model, val_loss = train_one_epoch_alternative(model, criterion, \n",
    "                                    optimizer1, train_dataloader, val_dataloader, device,\n",
    "                                    mode = nn_config['training']['opt_method'], \n",
    "                                    criterion_2= criterion_synthetic_samples, \n",
    "                                    dataloader_2 = synthetic_data_loader,\n",
    "                                    optimizer_2 = optimizer2, locked_masks2 = locked_masks2,\n",
    "                                    locked_masks = locked_masks, \n",
    "                                    repetitions = nn_config['training']['repetitions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e81f8f-9da0-47aa-b292-fa2eaab4ffa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e99956c89579991b3c1913705698ec2eedaae1b697d5c751470d306456d1f8fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
