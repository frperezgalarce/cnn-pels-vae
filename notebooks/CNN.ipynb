{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984721ea-2ed6-4933-a238-3a98989b3776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "new_directory = '/home/franciscoperez/Documents/GitHub/CNN-PELSVAE2/cnn-pels-vae/'\n",
    "os.chdir(new_directory)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from src.focalloss import  FocalLossMultiClass as focal_loss\n",
    "import src.utils as utils \n",
    "from src.cnn import initialize_masks, train_one_epoch_alternative, create_dataloader, setup_environment, initialize_optimizers\n",
    "from src.sampler.getbatch import SyntheticDataBatcher\n",
    "\n",
    "from src.utils import get_data\n",
    "import yaml \n",
    "import numpy as np\n",
    "from typing import Union, Tuple, Optional, Any, Dict, List\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.optim as optim\n",
    "\n",
    "with open('src/regressor.yaml', 'r') as file:\n",
    "    config_file: Dict[str, Any] = yaml.safe_load(file)\n",
    "vae_model: str =   config_file['model_parameters']['ID']  \n",
    "data_sufix: str =   config_file['model_parameters']['sufix_path']  \n",
    "\n",
    "with open('src/nn_config.yaml', 'r') as file:\n",
    "    nn_config = yaml.safe_load(file)\n",
    "    \n",
    "PP = utils.load_pp_list(vae_model)\n",
    "prior = False\n",
    "create_samples = False\n",
    "wandb_active = False\n",
    "N_LAYERS = 4\n",
    "opt_method= \"oneloss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a1c2924-0ecf-4e2d-bb1a-97bb59983213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 2, layers = 2) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=8, kernel_size=6, stride=1)\n",
    "        init.xavier_uniform_(self.conv1.weight)  \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=6, stride=1)\n",
    "        init.xavier_uniform_(self.conv2.weight)  \n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        if self.layers > 2: \n",
    "            self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=6, stride=1)\n",
    "            init.xavier_uniform_(self.conv3.weight)  \n",
    "            self.bn3 = nn.BatchNorm1d(32)\n",
    "            self.pool3 = nn.MaxPool1d(2)  \n",
    "        \n",
    "        if self.layers > 3: \n",
    "            self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=6, stride=1)\n",
    "            init.xavier_uniform_(self.conv4.weight)  \n",
    "            self.bn4 = nn.BatchNorm1d(64)\n",
    "            self.pool4 = nn.MaxPool1d(2)  \n",
    "\n",
    "        if self.layers == 2:\n",
    "            self.fc1 = nn.Linear(1136, 200)\n",
    "            init.xavier_uniform_(self.fc1.weight)  \n",
    "        elif self.layers == 3:\n",
    "            self.fc1 = nn.Linear(1056, 200)\n",
    "            init.xavier_uniform_(self.fc1.weight) \n",
    "        elif self.layers == 4:\n",
    "            self.fc1 = nn.Linear(896, 200)\n",
    "            init.xavier_uniform_(self.fc1.weight) \n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "        init.xavier_uniform_(self.fc2.weight)  \n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.pool1(x)        \n",
    "\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        if self.layers == 3: \n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.pool3(x)\n",
    "            \n",
    "        if self.layers == 4: \n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.pool3(x)\n",
    "            \n",
    "            x = self.conv4(x)\n",
    "            x = self.bn4(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.pool4(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)\n",
    "        x = F.tanh(x)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        \n",
    "        if (nn_config['training']['loss']=='NLLLoss') or (nn_config['training']['loss']=='focalLoss'):\n",
    "            return F.log_softmax(x, dim=1)  \n",
    "        else: \n",
    "            return x\n",
    "        \n",
    "def setup_model(num_classes: int, device: torch.device, show_architecture: bool = True) -> nn.Module:\n",
    "\n",
    "    print('----- model setup --------')\n",
    "    model = CNN(num_classes=num_classes, layers = N_LAYERS)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    if show_architecture:\n",
    "        print(\"Model Architecture:\")\n",
    "        print(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e139f786-bb3c-4dcd-94cf-ca2d69afcf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA active: True\n",
      "------ Data loading -------------------\n",
      "mode:  load 400000\n",
      "--------------------------------------------------\n",
      "MODE DATA:  load\n",
      "Loading light curves\n",
      "loaded files\n",
      "training data shape:  (205659, 300, 2)\n",
      "testing data shape:  (19484, 300, 2)\n",
      "Loaded shape training set:  (205659, 300, 2)\n",
      "Loaded shape testing set:  (19484, 300, 2)\n",
      "Modified shape training set:  (205659, 2, 300)\n",
      "Modified shape testing set:  (19484, 2, 300)\n",
      "Label to Number Mapping:\n",
      "CEP: 0\n",
      "DSCT: 1\n",
      "ECL: 2\n",
      "LPV: 3\n",
      "RRLYR: 4\n",
      "T2CEP: 5\n",
      "['CEP', 'DSCT', 'ECL', 'LPV', 'RRLYR', 'T2CEP']\n",
      "Training set\n",
      "num_classes:  6\n",
      "----- model setup --------\n",
      "Using 2 GPUs!\n",
      "Model Architecture:\n",
      "DataParallel(\n",
      "  (module): CNN(\n",
      "    (conv1): Conv1d(2, 8, kernel_size=(6,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv1d(8, 16, kernel_size=(6,), stride=(1,))\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv3): Conv1d(16, 32, kernel_size=(6,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv4): Conv1d(32, 64, kernel_size=(6,), stride=(1,))\n",
      "    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=896, out_features=200, bias=True)\n",
      "    (dropout1): Dropout(p=0.2)\n",
      "    (fc2): Linear(in_features=200, out_features=6, bias=True)\n",
      "    (dropout2): Dropout(p=0.2)\n",
      "  )\n",
      ")\n",
      "tensor([ 3.7534,  4.8470,  1.0024,  0.4808,  1.3360, 13.0587], device='cuda:0')\n",
      "Using mode: classic backpropagation\n",
      "oneloss False True\n",
      "Skipping synthetic sample creation\n",
      "oneloss False True\n",
      "Skipping synthetic sample creation\n"
     ]
    }
   ],
   "source": [
    "device = setup_environment()\n",
    "\n",
    "print('------ Data loading -------------------')\n",
    "print('mode: ', nn_config['data']['mode_running'], nn_config['data']['sample_size'])\n",
    "x_train, x_test, y_train, y_test, x_val, y_val, \\\n",
    "label_encoder, y_train_labeled, y_test_labeled = get_data(nn_config['data']['sample_size'], \n",
    "                                                          nn_config['data']['mode_running'])\n",
    "\n",
    "print(label_encoder)\n",
    "print('Training set')\n",
    "classes = np.unique(y_train_labeled.numpy())\n",
    "num_classes = len(classes)\n",
    "print('num_classes: ', num_classes)\n",
    "    \n",
    "model = setup_model(num_classes, device)\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train_labeled.numpy()), y_train_labeled.numpy())\n",
    "class_weights = np.sqrt(class_weights)\n",
    "class_weights = torch.tensor(class_weights).to(device, dtype=x_train.dtype)\n",
    "\n",
    "print(class_weights)\n",
    "if nn_config['training']['loss']=='CrossEntropyLoss':\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights) \n",
    "    criterion_synthetic_samples = nn.CrossEntropyLoss(weight=class_weights) \n",
    "elif nn_config['training']['loss']=='NLLLoss': \n",
    "    criterion = nn.NLLLoss(weight=class_weights) \n",
    "    criterion_synthetic_samples = nn.NLLLoss(weight=class_weights) \n",
    "elif  nn_config['training']['loss']=='focalLoss':\n",
    "    criterion = focal_loss(alpha=class_weights, gamma=2)\n",
    "    criterion_synthetic_samples = focal_loss(alpha=class_weights, gamma=2)      \n",
    "else: \n",
    "    raise('The required loss is not supported, '+ nn_config['training']['loss'])\n",
    "\n",
    "training_data = utils.move_data_to_device((x_train, y_train), device)\n",
    "val_data = utils.move_data_to_device((x_val, y_val), device)\n",
    "testing_data = utils.move_data_to_device((x_test, y_test), device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_f1_val = 0\n",
    "harder_samples = True\n",
    "no_improvement_count = 0\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "train_accuracy_values = []\n",
    "val_accuracy_values = []\n",
    "counter = 0\n",
    "\n",
    "epochs = nn_config['training']['epochs']\n",
    "patience =  nn_config['training']['patience']\n",
    "batch_size = nn_config['training']['batch_size']\n",
    "repetitions = nn_config['training']['repetitions']\n",
    "sinthetic_samples_by_class = nn_config['training']['sinthetic_samples_by_class']\n",
    "threshold_acc_synthetic = nn_config['training']['threshold_acc_synthetic'] \n",
    "beta_decay_factor= nn_config['training']['beta_decay_factor'] \n",
    "beta_initial= nn_config['training']['beta_initial'] \n",
    "EPS= nn_config['training']['EPS'] \n",
    "base_learning_rate= nn_config['training']['base_learning_rate'] \n",
    "scaling_factor= nn_config['training']['scaling_factor'] \n",
    "\n",
    "alpha = nn_config['training']['alpha']\n",
    "\n",
    "train_dataloader = create_dataloader(training_data, batch_size)\n",
    "val_dataloader = create_dataloader(val_data, batch_size)\n",
    "test_dataloader = create_dataloader(testing_data, batch_size)\n",
    "\n",
    "beta_actual = beta_initial\n",
    "\n",
    "optimizer1, optimizer2, locked_masks, locked_masks2 = initialize_optimizers(model, opt_method= opt_method,\n",
    "                                                                            EPS=EPS, base_learning_rate=base_learning_rate, \n",
    "                                                                            scaling_factor=scaling_factor)\n",
    "\n",
    "batcher = SyntheticDataBatcher(PP = PP, vae_model=vae_model, n_samples=sinthetic_samples_by_class, \n",
    "                                seq_length = x_train.size(-1), prior=prior)\n",
    "\n",
    "priorization = True\n",
    "for epoch in range(2):\n",
    "    print(opt_method, create_samples, harder_samples)\n",
    "    if opt_method=='twolosses' and create_samples and harder_samples: \n",
    "        if epoch>10 and priorization:\n",
    "            ranking, _ = get_dict_class_priorization(model, train_dataloader)\n",
    "            dict_priorization = {}\n",
    "            ranking_penalization = 2\n",
    "\n",
    "            for o in ranking:\n",
    "                dict_priorization[label_encoder[o]] =  int(sinthetic_samples_by_class*ranking_penalization)\n",
    "                if ranking_penalization>0.5:\n",
    "                    ranking_penalization = ranking_penalization/2\n",
    "\n",
    "            synthetic_data_loader = batcher.create_synthetic_batch(b=beta_actual, \n",
    "                                                                wandb_active=wandb_active, \n",
    "                                                                samples_dict = dict_priorization)\n",
    "        else: \n",
    "            synthetic_data_loader = batcher.create_synthetic_batch(b=beta_actual, \n",
    "                                                wandb_active=wandb_active, \n",
    "                                                samples_dict = None)\n",
    "\n",
    "        beta_actual = beta_actual*beta_decay_factor\n",
    "        harder_samples = False\n",
    "    elif  opt_method=='twolosses' and create_samples: \n",
    "        print(\"Using available synthetic data\")\n",
    "    else:\n",
    "        print(\"Skipping synthetic sample creation\")\n",
    "        synthetic_data_loader = None\n",
    "\n",
    "    running_loss, model, val_loss = train_one_epoch_alternative(model, criterion, optimizer1, train_dataloader, val_dataloader, device, \n",
    "                                    mode = opt_method, \n",
    "                                    criterion_2= criterion_synthetic_samples, \n",
    "                                    dataloader_2 = synthetic_data_loader,\n",
    "                                    optimizer_2 = optimizer2, locked_masks2 = locked_masks2, \n",
    "                                    locked_masks = locked_masks, repetitions = repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe45af5-6062-4e0c-9415-880393f4f69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e99956c89579991b3c1913705698ec2eedaae1b697d5c751470d306456d1f8fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
